{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hIbr52I7Z7U"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 1\n",
    "------------\n",
    "\n",
    "The objective of this assignment is to learn about simple data curation practices, and familiarize you with some of the data we'll be reusing later.\n",
    "\n",
    "This notebook uses the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset to be used with python experiments. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, while looking a little more like real data: it's a harder task, and the data is a lot less 'clean' than MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "apJbCsBHl-2A"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNWGtZaXn-5j"
   },
   "source": [
    "First, we'll download the dataset to our local machine. The data consists of characters rendered in a variety of fonts on a 28x28 image. The labels are limited to 'A' through 'J' (10 classes). The training set has about 500k and the testset 19000 labelled examples. Given these sizes, it should be possible to train models quickly on any machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186058,
     "status": "ok",
     "timestamp": 1444485672507,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "EYRJ4ICW6-da",
    "outputId": "0d0f85df-155f-4a89-8e7e-ee32df36ec8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified notMNIST_large.tar.gz\n",
      "Found and verified notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if force or not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cC3p0oEyF8QT"
   },
   "source": [
    "Extract the dataset from the compressed .tar.gz file.\n",
    "This should give you a set of directories, labelled A through J."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 186055,
     "status": "ok",
     "timestamp": 1444485672525,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "H8CBE-WZ8nmj",
    "outputId": "ef6c790c-2513-4b09-962e-27c79390c762"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large already present - Skipping extraction of notMNIST_large.tar.gz.\n",
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "notMNIST_small already present - Skipping extraction of notMNIST_small.tar.gz.\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['notMNIST_large/A',\n",
       " 'notMNIST_large/B',\n",
       " 'notMNIST_large/C',\n",
       " 'notMNIST_large/D',\n",
       " 'notMNIST_large/E',\n",
       " 'notMNIST_large/F',\n",
       " 'notMNIST_large/G',\n",
       " 'notMNIST_large/H',\n",
       " 'notMNIST_large/I',\n",
       " 'notMNIST_large/J']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'notMNIST_large/A/VGFyemFuYVdpZGUgSXRhbGljLnR0Zg==.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABxUlEQVR4nGWSPWgUURSFz/tZhwms\npIhoglhoYyyCLoKKjYUoGmtBC0XEQhEbRbBIo2CRRsEmCApb+QeChaRJJSlWRAsb8aeJEEVQC3fN\n7s7Me5/Fms3O5nSPw73n454nDcooeU0ILG+U0bCcLhGIFAfkhj2rLcuEQM4FeUm+vPXGRHCSUU1x\n3eD+LpFGEXm3LtGYBTIae3L4NTFE5HSOInDCfKTgeJnIaGyJjKfSc9pcL9M4zZLxZ6c0Q5dnpbVW\nUyvk3NIGHabgix90rV6Q8XlUTtua8HdSdmDpSULBaTkj1yDj1Fqo0egHMl7KSF4PaXN7DdfpJjkr\nNdnegTMWBgInm+TMykuy2kvgR3WVyOoxGUtjssYYq03fIKwW4zRNLDj//2k0T4eLPSKj9C05r5J0\nJE3TNK3qLh3m5LwkG67Ugtfur45eCqkq2meCJKsdv4lFHugrhEhzvEdTJ4+UFSPH5OXCoTNYc2dx\npF++bV072E2m5iW5RTLeVAYO7TVDl0eS02WKyFElvq9ERwh8SqSt38mpD3QgWY23oLVLukebn9tL\nppFv0OaspgMFV8ufQl4P6DDnN9fzavO+QsmMemKalff/AJyW/5++nFAdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "  data_folders = [\n",
    "    os.path.join(root, d) for d in sorted(os.listdir(root))\n",
    "    if os.path.isdir(os.path.join(root, d))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print(data_folders)\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)\n",
    "\n",
    "display(train_folders)\n",
    "train_folder = train_folders[0]\n",
    "image_files = os.listdir(train_folder)\n",
    "image_file = image_files[5]\n",
    "image_file_path = os.path.join(train_folder, image_file)\n",
    "display(image_file_path)\n",
    "Image(filename=image_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4riXK3IoHgx6"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Let's take a peek at some of the data to make sure it looks sensible. Each exemplar should be an image of a character A through J rendered in a different font. Display a sample of the images that we just downloaded. Hint: you can use the package IPython.display.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PBdkjESPK8tw"
   },
   "source": [
    "Now let's load the data in a more manageable format. Since, depending on your computer setup you might not be able to fit it all in memory, we'll load each class into a separate dataset, store them on disk and curate them independently. Later we'll merge them into a single dataset of manageable size.\n",
    "\n",
    "We'll convert the entire dataset into a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make training easier down the road. \n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 30
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 399874,
     "status": "ok",
     "timestamp": 1444485886378,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "h7q0XhG3MJdf",
    "outputId": "92c391bb-86ff-431d-9ada-315568a19e59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "notMNIST_large/A.pickle already present - Skipping pickling.\n",
      "notMNIST_large/B.pickle already present - Skipping pickling.\n",
      "notMNIST_large/C.pickle already present - Skipping pickling.\n",
      "notMNIST_large/D.pickle already present - Skipping pickling.\n",
      "notMNIST_large/E.pickle already present - Skipping pickling.\n",
      "notMNIST_large/F.pickle already present - Skipping pickling.\n",
      "notMNIST_large/G.pickle already present - Skipping pickling.\n",
      "notMNIST_large/H.pickle already present - Skipping pickling.\n",
      "notMNIST_large/I.pickle already present - Skipping pickling.\n",
      "notMNIST_large/J.pickle already present - Skipping pickling.\n",
      "notMNIST_small/A.pickle already present - Skipping pickling.\n",
      "notMNIST_small/B.pickle already present - Skipping pickling.\n",
      "notMNIST_small/C.pickle already present - Skipping pickling.\n",
      "notMNIST_small/D.pickle already present - Skipping pickling.\n",
      "notMNIST_small/E.pickle already present - Skipping pickling.\n",
      "notMNIST_small/F.pickle already present - Skipping pickling.\n",
      "notMNIST_small/G.pickle already present - Skipping pickling.\n",
      "notMNIST_small/H.pickle already present - Skipping pickling.\n",
      "notMNIST_small/I.pickle already present - Skipping pickling.\n",
      "notMNIST_small/J.pickle already present - Skipping pickling.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['notMNIST_large/A.pickle',\n",
       " 'notMNIST_large/B.pickle',\n",
       " 'notMNIST_large/C.pickle',\n",
       " 'notMNIST_large/D.pickle',\n",
       " 'notMNIST_large/E.pickle',\n",
       " 'notMNIST_large/F.pickle',\n",
       " 'notMNIST_large/G.pickle',\n",
       " 'notMNIST_large/H.pickle',\n",
       " 'notMNIST_large/I.pickle',\n",
       " 'notMNIST_large/J.pickle']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.48431373, -0.5       , -0.48039216,  0.0372549 ,  0.49215686,\n",
       "         0.5       ,  0.49607843,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.49607843,  0.5       ,  0.49607843,  0.05686275,\n",
       "        -0.48039216, -0.5       , -0.48431373],\n",
       "       [-0.5       , -0.32352942,  0.2764706 ,  0.5       ,  0.49607843,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.49607843,  0.5       ,\n",
       "         0.28431374, -0.31568629, -0.5       ],\n",
       "       [-0.01764706,  0.44901961,  0.5       ,  0.49215686,  0.49607843,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.48823529,\n",
       "         0.5       ,  0.45294118, -0.01372549],\n",
       "       [ 0.5       ,  0.49607843,  0.49215686,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.49215686,  0.48823529,\n",
       "         0.48823529,  0.48823529,  0.48823529,  0.48823529,  0.48823529,\n",
       "         0.48823529,  0.48823529,  0.48823529,  0.48823529,  0.49215686,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.49215686,  0.49607843,  0.5       ],\n",
       "       [ 0.48823529,  0.49607843,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.49607843,  0.48823529],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.49607843,  0.5       ,  0.34705883,  0.22941177,\n",
       "         0.25294119,  0.24901961,  0.24901961,  0.24901961,  0.24901961,\n",
       "         0.24901961,  0.24901961,  0.25294119,  0.22941177,  0.34705883,\n",
       "         0.5       ,  0.49607843,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.1       , -0.5       ,\n",
       "        -0.46862745, -0.48823529, -0.48823529, -0.48823529, -0.48823529,\n",
       "        -0.48823529, -0.48823529, -0.46862745, -0.5       , -0.1       ,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10392157, -0.5       ,\n",
       "        -0.47254902, -0.49215686, -0.49215686, -0.49215686, -0.49215686,\n",
       "        -0.49215686, -0.49215686, -0.47254902, -0.5       , -0.10392157,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.02156863, -0.4254902 ,\n",
       "        -0.3392157 , -0.35490197, -0.35490197, -0.35490197, -0.35490197,\n",
       "        -0.35490197, -0.35490197, -0.3392157 , -0.4254902 , -0.02156863,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.49215686,  0.48823529,\n",
       "         0.48823529,  0.48823529,  0.48823529,  0.48823529,  0.48823529,\n",
       "         0.48823529,  0.48823529,  0.48823529,  0.48823529,  0.49215686,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48431373,  0.5       ,  0.04509804, -0.31176472,\n",
       "        -0.23333333, -0.24901961, -0.24901961, -0.24901961, -0.24901961,\n",
       "        -0.24901961, -0.24901961, -0.23333333, -0.31176472,  0.04509804,\n",
       "         0.5       ,  0.48431373,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.1       , -0.5       ,\n",
       "        -0.46862745, -0.48823529, -0.48823529, -0.48823529, -0.48823529,\n",
       "        -0.48823529, -0.48823529, -0.46862745, -0.5       , -0.1       ,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ],\n",
       "       [ 0.5       ,  0.5       ,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.48039216,  0.5       , -0.10784314, -0.5       ,\n",
       "        -0.48039216, -0.5       , -0.5       , -0.5       , -0.5       ,\n",
       "        -0.5       , -0.5       , -0.48039216, -0.5       , -0.10784314,\n",
       "         0.5       ,  0.48039216,  0.5       ,  0.5       ,  0.5       ,\n",
       "         0.5       ,  0.5       ,  0.5       ]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "([array([  2.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,  25.]),\n",
       "  array([  1.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26.]),\n",
       "  array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,  26.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,  27.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,  15.,   1.,   1.,   0.,   0.,   1.,  10.]),\n",
       "  array([ 16.,   1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 15.,   1.,   1.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([ 16.,   1.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,  10.]),\n",
       "  array([  0.,   0.,   0.,  15.,   1.,   1.,   0.,   0.,   1.,  10.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  28.]),\n",
       "  array([  0.,   0.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,  27.]),\n",
       "  array([  1.,   0.,   0.,   0.,   0.,   0.,   0.,   1.,   0.,  26.]),\n",
       "  array([  1.,   1.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  26.]),\n",
       "  array([  2.,   0.,   0.,   0.,   1.,   0.,   0.,   0.,   0.,  25.])],\n",
       " array([-0.5, -0.4, -0.3, -0.2, -0.1,  0. ,  0.1,  0.2,  0.3,  0.4,  0.5]),\n",
       " <a list of 28 Lists of Patches objects>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAEACAYAAABRbNghAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84d46598d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  image_index = 0\n",
    "  print(folder)\n",
    "  for image in os.listdir(folder):\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "      image_index += 1\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  num_images = image_index\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)\n",
    "\n",
    "display(train_datasets)\n",
    "train_dataset_a = train_datasets[0]\n",
    "#train_label_a = train_labels[0]\n",
    "#display(train_label_a)\n",
    "f = open(train_dataset_a, 'rb')\n",
    "data = pickle.load(f)\n",
    "display(data[0])\n",
    "\n",
    "imgplot = plt.imshow(data[0])\n",
    "\n",
    "plt.hist(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUdbskYE2d87"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Let's verify that the data still looks good. Displaying a sample of the labels and images from the ndarray. Hint: you can use matplotlib.pyplot.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYznx5jUwzoO"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Another check: we expect the data to be balanced across classes. Verify that.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LA7M7K22ynCt"
   },
   "source": [
    "Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune `train_size` as needed. The labels will be stored into a separate array of integers 0 through 9.\n",
    "\n",
    "Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 411281,
     "status": "ok",
     "timestamp": 1444485897869,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "s3mWgZLpyuzq",
    "outputId": "8af66da6-902d-4719-bedc-7c9fb7ae7948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (200000, 28, 28) (200000,)\n",
      "Validation: (10000, 28, 28) (10000,)\n",
      "Testing: (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPTCnjIcyuKN"
   },
   "source": [
    "Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "6WZ2l2tN2zOL"
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puDUTe6t6USl"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "Convince yourself that the data is still good after shuffling!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIQJaJuwg5Hw"
   },
   "source": [
    "Finally, let's save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "QiR_rETzem6C"
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 413065,
     "status": "ok",
     "timestamp": 1444485899688,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "2a0a5e044bb03b66",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "hQbLjrW_iT39",
    "outputId": "b440efc6-5ee1-4cbc-d02d-93db44ebd956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed pickle size: 690800441\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)\n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gE_cRAQB33lk"
   },
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "By construction, this dataset might contain a lot of overlapping samples, including training data that's also contained in the validation and test set! Overlap between training and test can skew the results if you expect to use your model in an environment where there is never an overlap, but are actually ok if you expect to see training samples recur when you use it.\n",
    "Measure how much overlap there is between training, validation and test samples.\n",
    "\n",
    "Optional questions:\n",
    "- What about near duplicates between datasets? (images that are almost identical)\n",
    "- Create a sanitized validation and test set, and compare your accuracy on those in subsequent assignments.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Problem 5\n",
    "#f = open(pickle_file, 'rb')\n",
    "#saved_data = pickle.load(f)\n",
    "#train_dataset = saved_data['train_dataset']\n",
    "#test_dataset = saved_data['test_dataset']\n",
    "#valid_dataset = saved_data['valid_dataset']\n",
    "#seen = {}\n",
    "#for train in train_dataset:\n",
    "#    seen[hash(train)] = true\n",
    "#num_overlap = 0\n",
    "#for test in test_dataset:\n",
    "#    if seen[hash(test)]\n",
    "#        num_overlap++\n",
    "#display('hello')\n",
    "#display(num_overlap)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L8oww1s4JMQx"
   },
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Let's get an idea of what an off-the-shelf classifier can give you on this data. It's always good to check that there is something to learn, and that it's a problem that is not so trivial that a canned solution solves it.\n",
    "\n",
    "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. Hint: you can use the LogisticRegression model from sklearn.linear_model.\n",
    "\n",
    "Optional question: train an off-the-shelf model on all the data!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fitting for 50...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.480500%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 100...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.486500%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 1000...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.502500%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 2000...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.489500%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 3000...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.498000%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 4000...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.496000%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'fitting for 5000...'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'accuracy 0.493000%'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(samples, width, height) = train_dataset.shape\n",
    "X = np.reshape(train_dataset,(samples,width*height))\n",
    "(v_samples, v_width, v_height) = test_dataset.shape\n",
    "X_valid = np.reshape(test_dataset,(v_samples,v_width*v_height))\n",
    "\n",
    "for num_samples in [50, 100, 1000, 2000, 3000, 4000, 5000]:\n",
    "    fitX = X[:num_samples]\n",
    "    y = train_labels[:num_samples]\n",
    "\n",
    "    regr = LogisticRegression()\n",
    "    display(\"fitting for %d...\" % num_samples)\n",
    "    regr.fit(fitX, y)\n",
    "\n",
    "    prediction = regr.predict(X_valid)\n",
    "\n",
    "    num_correct = 0.0\n",
    "    for i in range(0, v_samples):\n",
    "        if prediction[i] == valid_labels[i]:\n",
    "            num_correct = num_correct + 1\n",
    "\n",
    "    display(\"accuracy %f%%\" % (num_correct / samples * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "1_notmnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
